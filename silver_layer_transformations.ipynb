{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82bdf79-b099-4edd-a984-183b9a26c91a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Silver Layer Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0452c99-bb65-43c3-a90f-9aef6d1645fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252b9f65-b679-447a-9de9-03be4768299b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:39:30.2219006Z",
       "execution_start_time": "2025-10-01T12:39:29.8983855Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2ce911a4-bf5d-4814-8a94-97c878705006",
       "queued_time": "2025-10-01T12:39:29.897265Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import PySpark libraries\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Configure timestamp handling for Fabric Runtime\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae20dba-7173-418f-bf70-f6b8b15ea8a0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Customers - SCD2 Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade458bd-022d-4bd3-a263-b7b2a7a59210",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:39:44.6543867Z",
       "execution_start_time": "2025-10-01T12:39:30.2240105Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2e6ac260-f445-44bf-9a26-89ebff810df1",
       "queued_time": "2025-10-01T12:39:29.9457199Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define schema for customer data\n",
    "customersSchema = StructType([\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"customer_name\", StringType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"signup_date\", DateType())\n",
    "])\n",
    "\n",
    "# Load customer data from Bronze layer\n",
    "df_customers = spark.read.format(\"csv\").option(\"header\", \"true\").schema(customersSchema).load(\"Files/bronze_customers/customers.csv\")\n",
    "\n",
    "# Data quality transformations\n",
    "df_customers = df_customers \\\n",
    "    .withColumn(\"customer_name\", when(col(\"customer_name\").isNull() | (col(\"customer_name\") == \"\"), lit(\"Unknown\")).otherwise(col(\"customer_name\"))) \\\n",
    "    .withColumn(\"email\", lower(trim(col(\"email\")))) \\\n",
    "    .withColumn(\"location\", when(col(\"location\").isNull() | (col(\"location\") == \"\"), lit(\"Unknown\")).otherwise(col(\"location\")))\n",
    "\n",
    "# Add SCD2 tracking columns and row hash for change detection\n",
    "current_ts = current_timestamp()\n",
    "high_date = lit(\"9999-12-31 23:59:59\").cast(\"timestamp\")\n",
    "\n",
    "df_customers = df_customers \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"valid_from\", current_ts) \\\n",
    "    .withColumn(\"valid_to\", high_date) \\\n",
    "    .withColumn(\"is_active\", lit(True)) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts) \\\n",
    "    .withColumn(\"row_hash\", hash(col(\"customer_name\"), col(\"email\"), col(\"location\")))\n",
    "\n",
    "# Create customers_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"customers_silver\") \\\n",
    "    .addColumn(\"customer_id\", StringType()) \\\n",
    "    .addColumn(\"customer_name\", StringType()) \\\n",
    "    .addColumn(\"email\", StringType()) \\\n",
    "    .addColumn(\"location\", StringType()) \\\n",
    "    .addColumn(\"signup_date\", DateType()) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"valid_from\", TimestampType()) \\\n",
    "    .addColumn(\"valid_to\", TimestampType()) \\\n",
    "    .addColumn(\"is_active\", BooleanType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .addColumn(\"row_hash\", LongType()) \\\n",
    "    .execute()\n",
    "\n",
    "# SCD2 implementation: Close old records on changes\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/customers_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_customers.alias('source'),\n",
    "        'target.customer_id = source.customer_id AND target.is_active = true'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.row_hash != source.row_hash\",\n",
    "        set={\n",
    "            \"is_active\": \"false\",\n",
    "            \"valid_to\": \"current_timestamp()\",\n",
    "            \"updated_at\": \"current_timestamp()\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Insert new active versions for changed customers\n",
    "df_customers.createOrReplaceTempView(\"temp_new_customers\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO customers_silver\n",
    "    SELECT source.*\n",
    "    FROM temp_new_customers source\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1\n",
    "        FROM customers_silver target\n",
    "        WHERE target.customer_id = source.customer_id\n",
    "            AND target.is_active = false\n",
    "            AND target.updated_at >= current_timestamp() - INTERVAL 10 SECONDS\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71be78-490f-4ede-bbda-1ffad432ddf5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Products - SCD2 Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652ddef-ddb7-4818-9dfa-001ff20e223d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for product data\n",
    "productsSchema = StructType([\n",
    "    StructField(\"product_key\", IntegerType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"stock\", IntegerType()),\n",
    "    StructField(\"unit_price\", DecimalType(10, 2)),\n",
    "    StructField(\"sales_price\", DecimalType(10, 2))\n",
    "])\n",
    "\n",
    "# Load product data from Bronze layer\n",
    "df_products = spark.read.format(\"csv\").option(\"header\", \"true\").schema(productsSchema).load(\"Files/bronze_products/products.csv\")\n",
    "\n",
    "# Data quality transformations\n",
    "df_products = df_products.drop(\"product_key\") \\\n",
    "    .withColumn(\"product_name\", when(col(\"product_name\").isNull() | (col(\"product_name\") == \"\"), lit(\"Unknown\")).otherwise(trim(col(\"product_name\")))) \\\n",
    "    .withColumn(\"category\", when(col(\"category\").isNull() | (col(\"category\") == \"\"), lit(\"Uncategorized\")).otherwise(trim(col(\"category\")))) \\\n",
    "    .withColumn(\"stock\", when(col(\"stock\").isNull(), lit(0)).otherwise(col(\"stock\"))) \\\n",
    "    .withColumn(\"unit_price\", when(col(\"unit_price\").isNull(), lit(0.00).cast(DecimalType(10, 2))).otherwise(col(\"unit_price\"))) \\\n",
    "    .withColumn(\"sales_price\", when(col(\"sales_price\").isNull(), lit(0.00).cast(DecimalType(10, 2))).otherwise(col(\"sales_price\"))) \\\n",
    "\n",
    "# Add SCD2 tracking columns and row hash for change detection\n",
    "current_ts = current_timestamp()\n",
    "high_date = lit(\"9999-12-31 23:59:59\").cast(\"timestamp\")\n",
    "\n",
    "df_products = df_products \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"valid_from\", current_ts) \\\n",
    "    .withColumn(\"valid_to\", high_date) \\\n",
    "    .withColumn(\"is_active\", lit(True)) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts) \\\n",
    "    .withColumn(\"row_hash\", hash(col(\"product_name\"), col(\"category\"), col(\"stock\"), col(\"unit_price\"), col(\"sales_price\")))\n",
    "\n",
    "# Create products_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"products_silver\") \\\n",
    "    .addColumn(\"product_id\", StringType()) \\\n",
    "    .addColumn(\"product_name\", StringType()) \\\n",
    "    .addColumn(\"category\", StringType()) \\\n",
    "    .addColumn(\"stock\", IntegerType()) \\\n",
    "    .addColumn(\"unit_price\", DecimalType(10, 2)) \\\n",
    "    .addColumn(\"sales_price\", DecimalType(10, 2)) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"valid_from\", TimestampType()) \\\n",
    "    .addColumn(\"valid_to\", TimestampType()) \\\n",
    "    .addColumn(\"is_active\", BooleanType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .addColumn(\"row_hash\", LongType()) \\\n",
    "    .execute()\n",
    "\n",
    "# SCD2 implementation: Close old records on changes\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/products_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_products.alias('source'),\n",
    "        'target.product_id = source.product_id AND target.is_active = true'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.row_hash != source.row_hash\",\n",
    "        set={\n",
    "            \"is_active\": \"false\",\n",
    "            \"valid_to\": \"current_timestamp()\",\n",
    "            \"updated_at\": \"current_timestamp()\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Insert new active versions for changed products\n",
    "df_products.createOrReplaceTempView(\"temp_new_products\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO products_silver\n",
    "    SELECT source.*\n",
    "    FROM temp_new_products source\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1\n",
    "        FROM products_silver target\n",
    "        WHERE target.product_id = source.product_id\n",
    "            AND target.is_active = false\n",
    "            AND target.updated_at >= current_timestamp() - INTERVAL 10 SECONDS\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb2db3-42b4-4db6-9adb-dbbf7fbad861",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Orders Header - Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17c1b3a-4082-494b-97e9-14195d474073",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:41:31.2818377Z",
       "execution_start_time": "2025-10-01T12:40:53.0673775Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "06ec9a72-a770-4277-b132-0c776d2e5320",
       "queued_time": "2025-10-01T12:39:30.2763401Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema for order header data\n",
    "ordersHeaderSchema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"order_date\", DateType()),\n",
    "    StructField(\"payment_method\", StringType()),\n",
    "    StructField(\"order_total\", DecimalType(12, 2))\n",
    "])\n",
    "\n",
    "# Load order header data from Bronze layer\n",
    "df_orders_header = spark.read.format(\"csv\").option(\"header\", \"true\").schema(ordersHeaderSchema).load(\"Files/bronze_pos_orders_header/orders_header.csv\")\n",
    "\n",
    "# Data quality transformations and derived attributes\n",
    "df_orders_header = df_orders_header \\\n",
    "    .withColumn(\"payment_method\", when(col(\"payment_method\").isNull() | (col(\"payment_method\") == \"\"), lit(\"Unknown\")).otherwise(upper(trim(col(\"payment_method\"))))) \\\n",
    "    .withColumn(\"order_total\", when(col(\"order_total\").isNull() | (col(\"order_total\") <= 0), lit(0.00).cast(DecimalType(12, 2))).otherwise(col(\"order_total\"))) \\\n",
    "\n",
    "# Add audit columns\n",
    "current_ts = current_timestamp()\n",
    "df_orders_header = df_orders_header \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts)\n",
    "\n",
    "# Create orders_header_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"orders_header_silver\") \\\n",
    "    .addColumn(\"order_id\", StringType()) \\\n",
    "    .addColumn(\"customer_id\", StringType()) \\\n",
    "    .addColumn(\"order_date\", DateType()) \\\n",
    "    .addColumn(\"payment_method\", StringType()) \\\n",
    "    .addColumn(\"order_total\", DecimalType(12, 2)) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .execute()\n",
    "\n",
    "# Idempotent merge: Update existing orders or insert new ones\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/orders_header_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_orders_header.alias('source'),\n",
    "        'target.order_id = source.order_id'\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.payment_method != source.payment_method OR target.order_total != source.order_total\",\n",
    "        set={\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"order_date\": \"source.order_date\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"order_total\": \"source.order_total\",\n",
    "            \"updated_at\": \"current_timestamp()\"\n",
    "        }\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7ffe5-9bc0-42c9-ac69-c2ba9719ae89",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Order Lines - Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12fcafca-1a61-415b-896d-3d943b9ee3a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:41:45.559308Z",
       "execution_start_time": "2025-10-01T12:41:31.2841768Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6811e36e-be3f-4cdc-a1be-c52b5e94b770",
       "queued_time": "2025-10-01T12:39:30.4430549Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema for order line data\n",
    "orderLinesSchema = StructType([\n",
    "    StructField(\"order_line_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"line_number\", IntegerType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"unit_price\", DecimalType(10, 2)),\n",
    "    StructField(\"discount_amount\", DecimalType(10, 2)),\n",
    "    StructField(\"tax_amount\", DecimalType(10, 2)),\n",
    "    StructField(\"line_total\", DecimalType(12, 2))\n",
    "])\n",
    "\n",
    "# Load order line data from Bronze layer\n",
    "df_order_lines = spark.read.format(\"csv\").option(\"header\", \"true\").schema(orderLinesSchema).load(\"Files/bronze_pos_order_lines/order_lines.csv\")\n",
    "\n",
    "# Data quality transformations\n",
    "df_order_lines = df_order_lines \\\n",
    "    .withColumn(\"quantity\", when(col(\"quantity\").isNull() | (col(\"quantity\") <= 0), lit(1)).otherwise(col(\"quantity\"))) \\\n",
    "    .withColumn(\"unit_price\", when(col(\"unit_price\").isNull(), lit(0.00).cast(DecimalType(10, 2))).otherwise(col(\"unit_price\"))) \\\n",
    "    .withColumn(\"discount_amount\", when(col(\"discount_amount\").isNull(), lit(0.00).cast(DecimalType(10, 2))).otherwise(col(\"discount_amount\"))) \\\n",
    "    .withColumn(\"tax_amount\", when(col(\"tax_amount\").isNull(), lit(0.00).cast(DecimalType(10, 2))).otherwise(col(\"tax_amount\"))) \\\n",
    "    .withColumn(\"line_total\", when(col(\"line_total\").isNull(), lit(0.00).cast(DecimalType(12, 2))).otherwise(col(\"line_total\")))\n",
    "\n",
    "# Add audit columns\n",
    "current_ts = current_timestamp()\n",
    "df_order_lines = df_order_lines \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts)\n",
    "\n",
    "# Create order_lines_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"order_lines_silver\") \\\n",
    "    .addColumn(\"order_line_id\", StringType()) \\\n",
    "    .addColumn(\"order_id\", StringType()) \\\n",
    "    .addColumn(\"line_number\", IntegerType()) \\\n",
    "    .addColumn(\"product_id\", StringType()) \\\n",
    "    .addColumn(\"quantity\", IntegerType()) \\\n",
    "    .addColumn(\"unit_price\", DecimalType(10, 2)) \\\n",
    "    .addColumn(\"discount_amount\", DecimalType(10, 2)) \\\n",
    "    .addColumn(\"tax_amount\", DecimalType(10, 2)) \\\n",
    "    .addColumn(\"line_total\", DecimalType(12, 2)) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .execute()\n",
    "\n",
    "# Idempotent merge: Update existing order lines or insert new ones\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/order_lines_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_order_lines.alias('source'),\n",
    "        'target.order_line_id = source.order_line_id'\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8691d2-53f4-4c3c-ba63-82c54810200b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Reviews - Immutable Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449501f-5162-4719-997f-1a50dc1da228",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:41:57.4199932Z",
       "execution_start_time": "2025-10-01T12:41:45.5614637Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "045becc9-8507-4aef-81dd-751cf87721d8",
       "queued_time": "2025-10-01T12:39:30.7129037Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema for review data\n",
    "reviewsSchema = StructType([\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"rating\", IntegerType()),\n",
    "    StructField(\"review_text\", StringType()),\n",
    "    StructField(\"timestamp\", LongType())\n",
    "])\n",
    "\n",
    "# Load review data from Bronze layer\n",
    "df_reviews = spark.read.schema(reviewsSchema).json(\"Files/bronze_reviews/reviews.json\")\n",
    "\n",
    "# Data quality filters\n",
    "df_reviews = df_reviews \\\n",
    "    .filter(col(\"customer_id\").isNotNull() & (col(\"customer_id\") != \"\")) \\\n",
    "    .filter(col(\"product_id\").isNotNull() & (col(\"product_id\") != \"\")) \\\n",
    "    .filter(col(\"rating\").isNotNull() & (col(\"rating\") >= 1) & (col(\"rating\") <= 5)) \\\n",
    "    .filter(col(\"timestamp\").isNotNull())\n",
    "\n",
    "# Transformations\n",
    "df_reviews = df_reviews \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"review_date\", col(\"timestamp\").cast(\"date\"))\n",
    "\n",
    "# Add audit columns\n",
    "current_ts = current_timestamp()\n",
    "df_reviews = df_reviews \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts)\n",
    "\n",
    "# Create reviews_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"reviews_silver\") \\\n",
    "    .addColumn(\"customer_id\", StringType()) \\\n",
    "    .addColumn(\"product_id\", StringType()) \\\n",
    "    .addColumn(\"timestamp\", TimestampType()) \\\n",
    "    .addColumn(\"review_date\", DateType()) \\\n",
    "    .addColumn(\"rating\", IntegerType()) \\\n",
    "    .addColumn(\"review_text\", StringType()) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .execute()\n",
    "\n",
    "# Idempotent merge: Prevent duplicate reviews\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/reviews_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_reviews.alias('source'),\n",
    "        'target.customer_id = source.customer_id AND target.product_id = source.product_id AND target.timestamp = source.timestamp'\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f00583-5d4e-4a52-bc2e-3ab08bfa9ada",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Social Media - Immutable Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223d9fc-6b6e-45ff-b414-49a8c77e756b",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T12:42:13.6522617Z",
       "execution_start_time": "2025-10-01T12:41:57.4222828Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4523f98e-104d-4596-9c95-2d6b17a9c6bf",
       "queued_time": "2025-10-01T12:39:30.9362215Z",
       "session_id": "eaa4542b-cb99-455d-b659-cf1ee5a6ab87",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, eaa4542b-cb99-455d-b659-cf1ee5a6ab87, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define schema for social media data\n",
    "socialMediaSchema = StructType([\n",
    "    StructField(\"content\", StringType()),\n",
    "    StructField(\"is_bot_like\", BooleanType()),\n",
    "    StructField(\"platform\", StringType()),\n",
    "    StructField(\"sentiment\", StringType()),\n",
    "    StructField(\"timestamp\", LongType())\n",
    "])\n",
    "\n",
    "# Load social media data from Bronze layer\n",
    "df_social_media = spark.read.schema(socialMediaSchema).json(\"Files/bronze_social_media/social_media.json\")\n",
    "\n",
    "# Data quality filters\n",
    "df_social_media = df_social_media \\\n",
    "    .filter(col(\"timestamp\").isNotNull()) \\\n",
    "    .filter(col(\"platform\").isNotNull() & (col(\"platform\") != \"\")) \\\n",
    "    .filter(col(\"content\").isNotNull() & (col(\"content\") != \"\")) \\\n",
    "    .filter(col(\"sentiment\").isNotNull() & col(\"sentiment\").isin([\"positive\", \"negative\", \"neutral\"]))\n",
    "\n",
    "# Transformations\n",
    "df_social_media = df_social_media \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"post_date\", col(\"timestamp\").cast(\"date\"))\n",
    "\n",
    "# Add audit columns\n",
    "current_ts = current_timestamp()\n",
    "df_social_media = df_social_media \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts)\n",
    "\n",
    "# Create social_media_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"social_media_silver\") \\\n",
    "    .addColumn(\"timestamp\", TimestampType()) \\\n",
    "    .addColumn(\"post_date\", DateType()) \\\n",
    "    .addColumn(\"platform\", StringType()) \\\n",
    "    .addColumn(\"sentiment\", StringType()) \\\n",
    "    .addColumn(\"content\", StringType()) \\\n",
    "    .addColumn(\"is_bot_like\", BooleanType()) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .execute()\n",
    "\n",
    "# Idempotent merge: Prevent duplicate posts\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/social_media_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_social_media.alias('source'),\n",
    "        'target.timestamp = source.timestamp AND target.platform = source.platform AND target.content = source.content'\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb77cc6-aee1-47dd-89c9-2fb7a6843c8f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Web Logs - Immutable Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc21f6f-0247-45d8-898a-3f2818cc41a9",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-01T13:02:48.9242104Z",
       "execution_start_time": null,
       "livy_statement_state": null,
       "normalized_state": "finished",
       "parent_msg_id": "c894b991-7f58-453b-a14c-c6ee949068ca",
       "queued_time": "2025-10-01T13:02:39.0765504Z",
       "session_id": null,
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": -1,
       "statement_ids": []
      },
      "text/plain": [
       "StatementMeta(, , -1, Finished, , Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LivyHttpRequestFailure",
     "evalue": "Something went wrong while processing your request. Please try again later. HTTP status code: 500. Trace ID: 2c86e573-aae4-4068-b2b0-4ca23741ac63.",
     "output_type": "error",
     "traceback": [
      "LivyHttpRequestFailure: Something went wrong while processing your request. Please try again later. HTTP status code: 500. Trace ID: 2c86e573-aae4-4068-b2b0-4ca23741ac63."
     ]
    }
   ],
   "source": [
    "# Define schema for web log data\n",
    "webLogsSchema = StructType([\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"page\", StringType()),\n",
    "    StructField(\"action\", StringType()),\n",
    "])\n",
    "\n",
    "# Load web log data from Bronze layer\n",
    "df_web_logs = spark.read.schema(webLogsSchema).json(\"Files/bronze_web_logs/web_logs.json\")\n",
    "\n",
    "# Data quality filters\n",
    "df_web_logs = df_web_logs \\\n",
    "    .filter(col(\"timestamp\").isNotNull()) \\\n",
    "    .filter(col(\"user_id\").isNotNull() & (col(\"user_id\") != \"\")) \\\n",
    "    .filter(col(\"page\").isNotNull() & (col(\"page\") != \"\")) \\\n",
    "    .filter(col(\"action\").isNotNull() & (col(\"action\") != \"\"))\n",
    "\n",
    "# Transformations\n",
    "df_web_logs = df_web_logs \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"timestamp\") / 1000).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"event_date\", col(\"timestamp\").cast(\"date\")) \\\n",
    "    .withColumnRenamed(\"user_id\", \"customer_id\") \\\n",
    "    .withColumn(\"page\", trim(col(\"page\"))) \\\n",
    "    .withColumn(\"action\", lower(trim(col(\"action\"))))\n",
    "\n",
    "# Add audit columns\n",
    "current_ts = current_timestamp()\n",
    "df_web_logs = df_web_logs \\\n",
    "    .withColumn(\"source_file_name\", input_file_name()) \\\n",
    "    .withColumn(\"created_at\", current_ts) \\\n",
    "    .withColumn(\"updated_at\", current_ts)\n",
    "\n",
    "# Create web_logs_silver table if not exists\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"web_logs_silver\") \\\n",
    "    .addColumn(\"timestamp\", TimestampType()) \\\n",
    "    .addColumn(\"event_date\", DateType()) \\\n",
    "    .addColumn(\"customer_id\", StringType()) \\\n",
    "    .addColumn(\"page\", StringType()) \\\n",
    "    .addColumn(\"action\", StringType()) \\\n",
    "    .addColumn(\"source_file_name\", StringType()) \\\n",
    "    .addColumn(\"created_at\", TimestampType()) \\\n",
    "    .addColumn(\"updated_at\", TimestampType()) \\\n",
    "    .execute()\n",
    "\n",
    "# Idempotent merge: Prevent duplicate events\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/web_logs_silver')\n",
    "\n",
    "deltaTable.alias('target') \\\n",
    "    .merge(\n",
    "        df_web_logs.alias('source'),\n",
    "        'target.customer_id = source.customer_id AND target.timestamp = source.timestamp'\n",
    "    ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "e3684ec9-f2e5-497d-bf80-6bfbf327eb23",
    "default_lakehouse_name": "MusicalMagic_Silver_LH",
    "default_lakehouse_workspace_id": "b257dd06-cc66-4335-b852-809cc1a2da56",
    "known_lakehouses": [
     {
      "id": "b435739e-4447-4738-90e8-051541e60278"
     },
     {
      "id": "e3684ec9-f2e5-497d-bf80-6bfbf327eb23"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
